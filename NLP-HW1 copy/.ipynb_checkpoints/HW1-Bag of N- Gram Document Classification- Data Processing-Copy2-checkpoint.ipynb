{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Documents \n",
    "import glob\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "from progressbar import ProgressBar\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "\n",
    "\n",
    "#Create File Index for Train and Val Data\n",
    "def create_index(path,num_val_samples):\n",
    "    pos = glob.glob(path+ \"pos/*\")\n",
    "    neg = glob.glob(path+ \"neg/*\")\n",
    "    pos = np.column_stack((pos,(np.ones(len(pos),dtype=int))))  # Label 1\n",
    "    neg = np.column_stack((neg,(np.zeros(len(neg),dtype=int))))  # Label 0\n",
    "    data= np.vstack((pos,neg))\n",
    "    \n",
    "    #Create Files Names For Val and Train Set\n",
    "    np.random.shuffle(data)\n",
    "    val, test = data[:num_val_samples,:], data[num_val_samples:,:]\n",
    "    \n",
    "    #Save files\n",
    "    np.savetxt(\"val_path.txt\",val[:,0], delimiter=\",\", fmt=\"%s\") \n",
    "    np.savetxt(\"val_target.txt\",val[:,1], delimiter=\",\", fmt= \"%s\")\n",
    "    np.savetxt(\"test_path.txt\",test[:,0], delimiter=\",\", fmt = \"%s\") \n",
    "    np.savetxt(\"test_target.txt\",test[:,1], delimiter=\",\", fmt= \"%s\") \n",
    "\n",
    "\n",
    "def read_data(files):\n",
    "    lines= []\n",
    "    for f in files:\n",
    "        with open(f) as file:\n",
    "            for line in file: \n",
    "                line = line.strip().lower().split()\n",
    "                lines.append(line) #storing everything in memory!\n",
    "                \n",
    "    return np.array(lines)\n",
    "        \n",
    "\n",
    "def load_data(path):\n",
    "    if isinstance(path, list):\n",
    "        data = read_data(path)\n",
    "        \n",
    "    else:\n",
    "        pos = read_data(glob.glob(path+ \"pos/*\"))\n",
    "        neg = read_data(glob.glob(path+ \"neg/*\"))\n",
    "            \n",
    "        #Create Labels for Pos and Negative Reviews\n",
    "        pos = np.column_stack((pos,(np.ones(len(pos),dtype=int))))  # Label 1\n",
    "        neg = np.column_stack((neg,(np.zeros(len(neg),dtype=int))))  # Label 0\n",
    "        data= np.vstack((pos,neg))\n",
    "        np.random.shuffle(data)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "#Training Data\n",
    "train_data = load_data(\"/Users/Taurean/Documents/NLP-HW1/data/train/\")\n",
    "x_train, y_train = train_data[:,0], train_data[:,1]\n",
    "\n",
    "#np.savetxt(\"train_target.txt\",y_train, delimiter=\",\", fmt= \"%s\")\n",
    "\n",
    "#Validation Data\n",
    "x_val = load_data(list(np.genfromtxt(\"/Users/Taurean/Documents/NLP-HW1/val_path.txt\",dtype='str')))\n",
    "y_val = np.genfromtxt(\"/Users/Taurean/Documents/NLP-HW1/target/val_target.txt\",dtype='int')\n",
    "\n",
    "#Test Data\n",
    "x_test = load_data(list(np.genfromtxt(\"/Users/Taurean/Documents/NLP-HW1/test_path.txt\",dtype='str')))\n",
    "y_test = np.genfromtxt(\"/Users/Taurean/Documents/NLP-HW1/target/test_target.txt\",dtype='int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in x_train is 25,000\n",
      "The number of samples in x_val is 5,000\n",
      "The number of samples in x_test is 20,000\n"
     ]
    }
   ],
   "source": [
    "#Print Total Samples in the data\n",
    "print(\"The number of samples in x_train is {:,d}\".format(len(x_train)))\n",
    "print(\"The number of samples in x_val is {:,d}\".format(len(x_val)))\n",
    "print(\"The number of samples in x_test is {:,d}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Schemes\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#Function to Process Review \n",
    "def clean_text(review,punctuation= False,stop_words= False, stem = False):\n",
    "    review= tokenizer(review)\n",
    "    if punctuation is True:\n",
    "        review = [token for token in review if (token.text not in punctuations)]\n",
    "        \n",
    "    if stop_words is True:\n",
    "        review = [token for token in review if (token.text not in stopwords)]\n",
    "        \n",
    "    return str(review)\n",
    "    \n",
    "\n",
    "#Clean Training Data   \n",
    "x_train_clean = []\n",
    "for x in tqdm(range(len(x_train))):\n",
    "    x_bow_rep =clean_text(str(x_train[x]),stop_words= True,punctuation= True, stem = False)\n",
    "    x_train_clean.append(x_bow_rep)\n",
    "    \n",
    "with open(\"train_tokens_clean\", \"wb\") as dill_file:\n",
    "    dill.dump(x_train_clean, dill_file)        \n",
    "    \n",
    "\n",
    "#Clean Val Data   \n",
    "x_val_clean = []\n",
    "for x in tqdm(range(len(x_val))):\n",
    "    x_bow_rep =clean_text(str(x_val[x]),stop_words= True,punctuation= True, stem = False)\n",
    "    x_val_clean.append(x_bow_rep)\n",
    "    \n",
    "with open(\"val_tokens_clean_test\", \"wb\") as dill_file:\n",
    "    dill.dump(x_val_clean, dill_file) \n",
    "    \n",
    "    \n",
    "    \n",
    "#Clean Text Data\n",
    "x_test_clean = []\n",
    "for x in tqdm(range(len(x_test))):\n",
    "    x_bow_rep =clean_text(str(x_test[x]),stop_words= True,punctuation= True, stem = False)\n",
    "    x_test_clean.append(x_bow_rep)\n",
    "\n",
    "with open(\"test_tokens_clean\", \"wb\") as dill_file:\n",
    "    dill.dump(x_test_clean, dill_file)        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better',\n",
       " 'than',\n",
       " 'the',\n",
       " 'typical',\n",
       " 'made-for-tv',\n",
       " 'movie,',\n",
       " 'invitation',\n",
       " 'to',\n",
       " 'hell',\n",
       " 'is',\n",
       " 'blessed',\n",
       " 'with',\n",
       " 'excellent',\n",
       " 'casting',\n",
       " '(urich,',\n",
       " 'lucci,',\n",
       " 'cassidy,',\n",
       " 'mccarthy,',\n",
       " 'pre-murphy',\n",
       " 'brown',\n",
       " 'joe',\n",
       " 'regalbuto,',\n",
       " 'soleil',\n",
       " 'moon-frye)',\n",
       " 'and',\n",
       " 'a',\n",
       " 'high',\n",
       " 'concept',\n",
       " 'update',\n",
       " 'to',\n",
       " 'the',\n",
       " 'familiar',\n",
       " 'faustian',\n",
       " 'plot.',\n",
       " 'urich',\n",
       " 'is',\n",
       " 'likable',\n",
       " 'as',\n",
       " 'always',\n",
       " 'and',\n",
       " 'lucci',\n",
       " 'is',\n",
       " 'particularly',\n",
       " 'fetching',\n",
       " 'and',\n",
       " 'devilishly',\n",
       " 'over',\n",
       " 'the',\n",
       " 'top',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mother',\n",
       " 'of',\n",
       " 'all',\n",
       " 'femme',\n",
       " 'fatale',\n",
       " 'roles.',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'a',\n",
       " 'hybrid',\n",
       " 'version',\n",
       " 'of',\n",
       " 'stepford',\n",
       " 'wives',\n",
       " 'and',\n",
       " 'they',\n",
       " 'live,',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'commits',\n",
       " 'early',\n",
       " 'to',\n",
       " 'its',\n",
       " 'apocalyptic',\n",
       " 'miltonesque',\n",
       " 'vision',\n",
       " 'and',\n",
       " 'horror',\n",
       " 'fans',\n",
       " 'will',\n",
       " 'likely',\n",
       " 'not',\n",
       " 'have',\n",
       " 'many',\n",
       " 'complaints',\n",
       " 'until',\n",
       " 'the',\n",
       " 'soppy,',\n",
       " 'maudlin',\n",
       " 'denoument.',\n",
       " '7/10']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/Taurean/Documents/NLP-HW1/tokens/test_tokens\", \"rb\") as dill_file:\n",
    "    y=dill.load(dill_file)\n",
    "    \n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maps Word to Id Number\n",
    "def data_dictionary(reviews,vocab_size_limit):\n",
    "    token_counter = Counter()\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    for review in reviews:\n",
    "        for words in set(review):\n",
    "            token_counter[words] += 1\n",
    "            \n",
    "\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size_limit))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#Convert Review tokens to Id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else 0 for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "\n",
    "\n",
    "x= data_dictionary(all_tags,200)\n",
    "token2id= x[0]\n",
    "id2token = x[1]\n",
    "train_data_indices = token2index_dataset(x_train)\n",
    "val_data_indices = token2index_dataset(x_val)\n",
    "test_data_indices = token2index_dataset(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.stack((x_train,x_val,x_test))\n",
    "all_tags=np.concatenate((x_train, x_val, x_test), axis=None)\n",
    "len(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "list_ = []\n",
    "data=list(x_train)\n",
    "for x in range(len(data)):\n",
    "    line =find_ngrams(data[x],3)\n",
    "    list_.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   6,   0,  ...,   0,  14,  12],\n",
      "        [ 71,   0, 122,  ...,   0,   9,   2],\n",
      "        [ 51,   0, 134,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 12,   0,   8,  ...,   0,  17,   0],\n",
      "        [ 12,  33,   8,  ...,   4,   0,   3],\n",
      "        [  3,   0,   0,  ...,   0,   0,   0]])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB movie review tokens \n",
    "        @param target_list: list of IMDB movie review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMBD_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDBDataset(train_data_indices, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMBD_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(val_data_indices, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         collate_fn=IMBD_collate_func,\n",
    "                                         shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          collate_fn=IMBD_collate_func,\n",
    "                                          shuffle=False)\n",
    "\n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (data)\n",
    "    print (labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/782], Validation Acc: 66.96, Training Acc: 67.064\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 69.32, Training Acc: 69.896\n",
      "Epoch: [1/10], Step: [301/782], Validation Acc: 72.0, Training Acc: 72.148\n",
      "Epoch: [1/10], Step: [401/782], Validation Acc: 73.2, Training Acc: 72.98\n",
      "Epoch: [1/10], Step: [501/782], Validation Acc: 73.62, Training Acc: 73.596\n",
      "Epoch: [1/10], Step: [601/782], Validation Acc: 73.92, Training Acc: 73.94\n",
      "Epoch: [1/10], Step: [701/782], Validation Acc: 73.64, Training Acc: 73.636\n",
      "Epoch: [2/10], Step: [101/782], Validation Acc: 74.46, Training Acc: 73.948\n",
      "Epoch: [2/10], Step: [201/782], Validation Acc: 73.02, Training Acc: 72.816\n",
      "Epoch: [2/10], Step: [301/782], Validation Acc: 74.26, Training Acc: 73.52\n",
      "Epoch: [2/10], Step: [401/782], Validation Acc: 73.5, Training Acc: 73.52\n",
      "Epoch: [2/10], Step: [501/782], Validation Acc: 73.44, Training Acc: 73.5\n",
      "Epoch: [2/10], Step: [601/782], Validation Acc: 73.78, Training Acc: 73.716\n",
      "Epoch: [2/10], Step: [701/782], Validation Acc: 74.64, Training Acc: 74.056\n",
      "Epoch: [3/10], Step: [101/782], Validation Acc: 73.34, Training Acc: 72.992\n",
      "Epoch: [3/10], Step: [201/782], Validation Acc: 74.06, Training Acc: 73.644\n",
      "Epoch: [3/10], Step: [301/782], Validation Acc: 73.7, Training Acc: 73.348\n",
      "Epoch: [3/10], Step: [401/782], Validation Acc: 74.12, Training Acc: 73.716\n",
      "Epoch: [3/10], Step: [501/782], Validation Acc: 72.68, Training Acc: 72.888\n",
      "Epoch: [3/10], Step: [601/782], Validation Acc: 74.5, Training Acc: 74.108\n",
      "Epoch: [3/10], Step: [701/782], Validation Acc: 72.38, Training Acc: 72.096\n",
      "Epoch: [4/10], Step: [101/782], Validation Acc: 73.42, Training Acc: 73.592\n",
      "Epoch: [4/10], Step: [201/782], Validation Acc: 73.8, Training Acc: 73.76\n",
      "Epoch: [4/10], Step: [301/782], Validation Acc: 74.2, Training Acc: 73.76\n",
      "Epoch: [4/10], Step: [401/782], Validation Acc: 74.5, Training Acc: 73.912\n",
      "Epoch: [4/10], Step: [501/782], Validation Acc: 73.18, Training Acc: 72.988\n",
      "Epoch: [4/10], Step: [601/782], Validation Acc: 73.7, Training Acc: 73.604\n",
      "Epoch: [4/10], Step: [701/782], Validation Acc: 74.28, Training Acc: 73.876\n",
      "Epoch: [5/10], Step: [101/782], Validation Acc: 74.48, Training Acc: 74.064\n",
      "Epoch: [5/10], Step: [201/782], Validation Acc: 72.98, Training Acc: 73.008\n",
      "Epoch: [5/10], Step: [301/782], Validation Acc: 71.1, Training Acc: 70.996\n",
      "Epoch: [5/10], Step: [401/782], Validation Acc: 73.92, Training Acc: 73.784\n",
      "Epoch: [5/10], Step: [501/782], Validation Acc: 74.22, Training Acc: 73.872\n",
      "Epoch: [5/10], Step: [601/782], Validation Acc: 74.14, Training Acc: 74.016\n",
      "Epoch: [5/10], Step: [701/782], Validation Acc: 74.44, Training Acc: 73.976\n",
      "Epoch: [6/10], Step: [101/782], Validation Acc: 74.28, Training Acc: 73.78\n",
      "Epoch: [6/10], Step: [201/782], Validation Acc: 74.18, Training Acc: 73.56\n",
      "Epoch: [6/10], Step: [301/782], Validation Acc: 74.0, Training Acc: 73.76\n",
      "Epoch: [6/10], Step: [401/782], Validation Acc: 74.58, Training Acc: 74.16\n",
      "Epoch: [6/10], Step: [501/782], Validation Acc: 74.52, Training Acc: 73.94\n",
      "Epoch: [6/10], Step: [601/782], Validation Acc: 74.48, Training Acc: 74.028\n",
      "Epoch: [6/10], Step: [701/782], Validation Acc: 74.14, Training Acc: 73.868\n",
      "Epoch: [7/10], Step: [101/782], Validation Acc: 74.08, Training Acc: 73.512\n",
      "Epoch: [7/10], Step: [201/782], Validation Acc: 74.16, Training Acc: 74.128\n",
      "Epoch: [7/10], Step: [301/782], Validation Acc: 73.74, Training Acc: 73.532\n",
      "Epoch: [7/10], Step: [401/782], Validation Acc: 74.12, Training Acc: 73.976\n",
      "Epoch: [7/10], Step: [501/782], Validation Acc: 73.78, Training Acc: 73.728\n",
      "Epoch: [7/10], Step: [601/782], Validation Acc: 74.6, Training Acc: 74.032\n",
      "Epoch: [7/10], Step: [701/782], Validation Acc: 74.32, Training Acc: 74.116\n",
      "Epoch: [8/10], Step: [101/782], Validation Acc: 74.38, Training Acc: 74.044\n",
      "Epoch: [8/10], Step: [201/782], Validation Acc: 73.9, Training Acc: 73.724\n",
      "Epoch: [8/10], Step: [301/782], Validation Acc: 73.72, Training Acc: 73.572\n",
      "Epoch: [8/10], Step: [401/782], Validation Acc: 74.3, Training Acc: 73.86\n",
      "Epoch: [8/10], Step: [501/782], Validation Acc: 73.68, Training Acc: 73.42\n",
      "Epoch: [8/10], Step: [601/782], Validation Acc: 73.94, Training Acc: 73.672\n",
      "Epoch: [8/10], Step: [701/782], Validation Acc: 74.62, Training Acc: 74.176\n",
      "Epoch: [9/10], Step: [101/782], Validation Acc: 74.56, Training Acc: 74.236\n",
      "Epoch: [9/10], Step: [201/782], Validation Acc: 73.58, Training Acc: 73.516\n",
      "Epoch: [9/10], Step: [301/782], Validation Acc: 72.72, Training Acc: 72.956\n",
      "Epoch: [9/10], Step: [401/782], Validation Acc: 74.6, Training Acc: 73.944\n",
      "Epoch: [9/10], Step: [501/782], Validation Acc: 74.06, Training Acc: 73.988\n",
      "Epoch: [9/10], Step: [601/782], Validation Acc: 73.48, Training Acc: 73.588\n",
      "Epoch: [9/10], Step: [701/782], Validation Acc: 74.62, Training Acc: 73.952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-17d0949b4d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Acc: {}'.format( \n",
      "\u001b[0;32m<ipython-input-70-17d0949b4d6e>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlpclass/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-80e5aceed3d0>\u001b[0m in \u001b[0;36mIMBD_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m         padded_vec = np.pad(np.array(datum[0]), \n\u001b[1;32m     51\u001b[0m                                 \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                 mode=\"constant\", constant_values=0)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepend_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'edge'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlpclass/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_append_const\u001b[0;34m(arr, pad_amt, val, axis)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     padshape = tuple(x if i != axis else pad_amt\n\u001b[0;32m--> 160\u001b[0;31m                      for (i, x) in enumerate(arr.shape))\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_do_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            \n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 87.1\n",
      "Test Acc 87.23\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "list_ = []\n",
    "data=list(x_train)\n",
    "for x in range(len(data)):\n",
    "    line =find_ngrams(data[x],3)\n",
    "    list_.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
